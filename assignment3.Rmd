---
title: "Assignment 3"
author: "Suyeon Ryu"
date: '2020 10 14'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# APIs

## Finding "sars-cov-2 trail vaccine"

```{r}
#downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2%20trial%20vaccine.")

# Finding the counts
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/span")

# Turning it into text
counts <- as.character(counts)

# Extracting the data using regex
stringr::str_extract(counts, "[0-9,]+")

```
From NCBI API, I could find that there are total 564 results with the search term "sars-cov-2 trial vaccine."

## download each papers' details
```{r}

library(httr)
query_ids <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
  query = list(
    db = "pubmed",
    term = "sars-cov-2 trial vaccine",
    retype = "abstract",
    retmax=1000
               )
)


```

```{r}
# Extracting the content from the response 

ids <- httr::content(query_ids)

```


## create a dataset 
```{r,echo=TRUE,include=FALSE}
# Turn the result into a character vector
ids <- as.character(ids)
cat(ids)


# Find all the ids 
ids <- stringr::str_extract_all(ids,"<Id>[0-9]+</Id>")[[1]]

# Remove all the leading and trailing <Id> </Id>. Make use of "|"
ids <- stringr::str_remove_all(ids, "<Id>|</Id>")
```

```{r}
publications <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
  query = list(
      db = "pubmed",
      id = paste(ids, collapse = ","),
      retmax = 1000,
      rettype = "abstract"
    
    )
)

# Turning the output into character vector
publications <- httr::content(publications)
publications_txt <- as.character(publications)

```



```{r}
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)

```

```{r}
##extract "Abstracts"
library(stringr)

abstracts <- str_extract(pub_char_list, "<Abstract>(\\n|.)+</Abstract>")
abstracts <- str_remove_all(abstracts, "</?[[:alnum:]]+>")
abstracts <- str_replace_all(abstracts, "\\s+", " ")

## extract title
titles <- str_extract(pub_char_list, "<ArticleTitle>(\\n|.)+</ArticleTitle>")
titles <- str_remove_all(titles, "</?[[:alnum:]]+>")
titles <- str_replace_all(titles, "\\s+"," ")

##extract journal name
journal <- str_extract(pub_char_list, "<Title>(\\n|.)+</Title>")
journal <- str_remove_all(journal, "</?[[:alnum:]]+>")
journal <- str_replace_all(journal, "\\s+"," ") 

## extract the dates
year <- str_extract(pub_char_list, "<Year>[0-9]+</Year>")
year <- str_remove_all(year, "</?[[:alnum:]]+>")
year <- str_replace_all(year, "\\s+"," ") 

month <- str_extract(pub_char_list, "<Month>[0-9]+</Month>")
month <- str_remove_all(month, "</?[[:alnum:]]+>")
month <- str_replace_all(month, "\\s+"," ") 

day <- str_extract(pub_char_list, "<Day>[0-9]+</Day>")
day <- str_remove_all(day, "</?[[:alnum:]]+>")
day <- str_replace_all(day, "\\s+"," ")




```

```{r}
## put together everything

database <- data.frame(
  PumMedID = ids,
  Title = titles,
  Journal = journal,
  PubYear = year,
  PubMonth = month,
  PubDay = day,
  Abstracts = abstracts
)
knitr::kable(database) 

```





# Text Mining 

## 1. Tokenize abstract and count 

```{r}
library(tidyverse)
library(ggplot2)
library(tidytext)
library(dplyr)
library(readr)


```

```{r}

pubmed <- readr::read_csv("C:/Users/suyeo/PM566/assignment3/pubmed.csv")

covid <- pubmed %>%
  select(abstract, term)

```

```{r}
covid %>%
  unnest_tokens(token, term) %>%
  count(token, sort = TRUE)

covid %>%
  unnest_tokens(token, abstract) %>%
  count(token, sort = TRUE)
```

There are a lot of stop words included, and I can see covid and 19 are separated and considered as two tokens in abstracts. 

```{r}
covid %>%
  unnest_tokens(token, term) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  count(token, sort = TRUE)

covid %>%
  unnest_tokens(token, abstract) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  count(token, sort = TRUE)
```

After removal, the stop words are gone and more relevant words tokens are left. 
The most common tokens after removing stop-words are : covid, 19, patients, cancer, prostate.

## 2. Tokenize abstracts into bigrams. 

```{r}

covid %>%
  unnest_ngrams(ngram, abstract, n=2) %>%
  count(ngram, sort = TRUE) %>%
  top_n(10, n)%>%
  ggplot(aes(n, fct_reorder(ngram, n))) +
  geom_col()

```


Based on the tokenized abstracts bigram above, the 10 most frequent bigrams are shown. The most frequent bigram is covid 19, which is somewhat expected. 


## 3. Calculate TF-IDF value

```{r}


covid %>%
  unnest_tokens(document, abstract) %>%
  anti_join(stop_words, by = c("document" = "word")) %>%
  count(document, sort = TRUE)%>%
  bind_tf_idf(document,document, n)%>%
  arrange(desc(tf_idf))





```

